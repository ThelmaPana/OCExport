---
title: "Plankton anomalies to POC anomalies"
subtitle: "Use plankton residuals to try to predict POC residuals."
format:
  html:
    toc: true
editor: visual
---

## Set-up and load data

```{r}
#| output: false
#| warning: false
source("utils.R")
#load("data/04.all_data.Rdata")
load("data/07.poc_res.Rdata")
load("data/08.plankton_res_mbtr.Rdata")

df <- poc_res %>% 
  rename(poc_res = residuals) %>% 
  left_join(plankton_res %>% select(-datetime), by = join_by(lon, lat)) # remove datetime from residuals computed in python because the datetime is messed up
```

## Data inspection

### Response variable

Let’s have a look at our response variable: POC value. We do not need to inspect response variables as the model we are going to use are robust to non-normal distributions.

```{r}
df %>% ggplot() + geom_histogram(aes(x = poc_res), bins = 100)
```

Since these are residuals, they have a nice normal distribution

### Explanatory variables

Select explanatory variables.

```{r}
# Explanatory variables
exp_vars <- df %>% select(ta_ric:m_eve) %>% colnames()
# Metadata variables
meta_vars <- df %>% select(lon, lat, datetime) %>% colnames()
```

Let’s run a quick PCA. This allows us to have at correlations within our datasets as well as the main axes of variance.

```{r}
# Need to remove lon and lat and to scale because units differ between variables
df_pca <- df %>% select(poc_res, all_of(exp_vars))
pca_all <- FactoMineR::PCA(df_pca, quanti.sup = 1, scale.unit = TRUE, graph = FALSE)
```

Plot eigenvalues.

```{r}
eig <- pca_all$eig %>%
  as.data.frame() %>%
  rownames_to_column(var = "comp") %>%
  as_tibble() %>%
  mutate(
    comp = str_remove(comp, "comp "),
    comp = as.numeric(comp),
    comp = as.factor(comp)
  ) %>%
  rename(var = `percentage of variance`, cum_var = `cumulative percentage of variance`)

eig %>%
  ggplot() +
  geom_col(aes(x = comp, y = var)) +
  theme_classic() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "PC", y = "Explained variance")
```

Plot first plane.

```{r}
#| fig-column: body-outset
#| out-width: 100%
plot(pca_all, choix="var", axes = c(1, 2))
```

The two first axes are:

-   PC1: oxygen rich in positive values VS warm in negative values

-   PC2: mixed waters in positive values VS stratified waters in negative values

Extract coordinates in the PCA space.

```{r}
## Get coordinates of individuals
inds <- pca_all$ind$coord %>% as_tibble()
# Set nice names for columns
colnames(inds) <- str_c("dim", paste(c(1:ncol(inds))))
# And join with initial dataframe of objects
inds <- df %>%
  bind_cols(inds)
```

Plot maps of profiles.

```{r}
#| fig-column: body-outset
#| out-width: 100%
ggmap(inds, "dim1", type = "point", palette = div_pal)
```

```{r}
#| fig-column: body-outset
#| out-width: 100%
ggmap(inds, "dim2", type = "point", palette = div_pal)
```

## Data split

Let’s take 80% of the data to fit the model, and keep 20% to test the model at the very end. We stratify our split by `poc_log`, the response value, so that both training and testing sets are representative of the whole range of values.

```{r}
# Train VS test, stratified
df_split <- initial_split(df, prop = 0.8, strata = poc_res)
df_train <- training(df_split)
df_test <- testing(df_split)
```

To evaluate model performance during training, we’ll use cross-validation on the training set, with 10 folds, again stratified by `poc_log`.

```{r}
# Cross-validation, 10 folds, stratified
df_folds <- vfold_cv(df_train, v = 10, strata = poc_res)
```

## Model definition

Let’s define a XGBoost regression model, with tunable hyperparameters:

-   `trees`: number of trees

-   `tree_depth`: maximum depth (i.e. number of splits) in a tree

-   `min_n`: minimum number of objects in a node to split further

-   `learn_rate`

```{r}
# Define a xgboost model with hyperparameters to tune
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  #loss_reduction = tune(),
  #sample_size = tune(),
  #mtry = tune(),
  learn_rate = tune()
) %>%
  set_mode("regression") %>%
  set_engine("xgboost", num.threads = n_cores)
extract_parameter_set_dials(xgb_spec)
```

The model will be run on `r n_cores` cores in parallel.

## Recipe

Let’s use a recipe to define the model formula and assign roles to variables. We want to predict `poc_log` from all other variables expect `lon`, `lat` and original `poc` values, yet it is convenient to keep these in the dataset.

```{r}
# Generate formula from list of explanatory variables
# NB: we also add metadata variables and untransformed "poc" for technical reasons but these will not be used to fit the model
xgb_form <- as.formula(paste("poc_res ~ ", paste(c(meta_vars, exp_vars), collapse = " + "), sep = ""))

xgb_rec <- recipe(xgb_form, data = df_train) %>%
  update_role(all_of(meta_vars), new_role = "coords") # These variables can be retained in the data but not included in the model
summary(xgb_rec)
```

## Workflow

Set-up the workflow with the recipe and the model.

```{r}
xgb_wflow <- workflow() %>%
  add_recipe(xgb_rec) %>%
  add_model(xgb_spec)
```

## Gridsearch

It is now time to perform the gridsearch to find the best hyperparameters for our model. We first need to define a grid with the hyperparameters we want to try. We use `grid_latin_hypercube` to generate a space-filling grid of size `30`.

```{r}
xgb_grid <- grid_latin_hypercube(
  trees(),
  learn_rate(),
  tree_depth(),
  min_n(),
  #loss_reduction(),
  #sample_size = sample_prop(),
  #finalize(mtry(), df_train),
  size = 30
)
xgb_grid
```

Tune the grid to find the best hyperparameters, in parallel. This is done on our cross-validation folds: for each fold, a model is fitted on training data and evaluated on validation data.

```{r}
doParallel::registerDoParallel()
xgb_res <- tune_grid(
  xgb_wflow,
  resamples = df_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)
autoplot(xgb_res)
```

Select the best set of hyperparameters based on `rmse` value.

```{r}
show_best(xgb_res, metric = "rmse")
best_xgb <- select_best(xgb_res, metric = "rmse")
```

## Finalize

Let’s now finalize our model with the selected set of hyperparameters.

```{r}
final_xgb <- finalize_workflow(
  xgb_wflow,
  best_xgb
)
```

Fit the model on training data.

```{r}
final_res <- fit(final_xgb, df_train)
```

Predict test data.

```{r}
preds <- augment(final_res, new_data = df_test) 
```

## Model evaluation

### Prediction metrics

Let’s have a look at prediction metrics on the test set.

```{r}
rmse(preds, truth = poc_res, estimate = .pred)
rsq(preds, truth = poc_res, estimate = .pred)
```

### Prediction VS truth

Plot predictions VS truth, both in the log-transformed space and in the non log-transformed space.

```{r}
preds %>%
  ggplot() +
  geom_point(aes(x = poc_res, y = .pred)) +
  geom_abline(slope = 1, color = "red") +
  labs(title = "Pred VS truth")
```

## Extract residuals

Finally, let’s extract residuals, have a look at them and save them.

```{r}
preds %>%
  mutate(residuals = .pred - poc_res) %>%
  ggplot() +
  geom_density(aes(x = residuals)) +
  labs(x = "Residuals")
```

## Model interpretation

Let’s feed our model to `explain_tidymodel`, but first we need to select predictors only.

```{r}
# Select only predictors
vip_train <- xgb_rec %>% prep() %>% bake(new_data = NULL, all_predictors())


# Explainer
xgb_explain <-
  explain_tidymodels(
    model = extract_fit_parsnip(final_res),
    data = vip_train,
    y = df_train %>%  pull(poc_res)
  )
```

### Variable importance

```{r}
xgb_var_imp <- model_parts(xgb_explain)
ggplot_imp(xgb_var_imp)
```
