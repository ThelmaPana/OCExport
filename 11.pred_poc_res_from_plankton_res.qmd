---
title: "Predict POC residuals from plankton residuals"
subtitle: "Train a XGBoost model to predict POC residuals (from env) from plankton residuals (from env)."
author: "Thelma Panaïotis"
format:
  html:
    toc: true
    embed-resources: true
editor: visual
execute:
  cache: true
  warning: false
---

## Set-up and load data

```{r set_up}
#| output: false
#| cache: false
source("utils.R")

# POC residuals from env
load("data/ ")
poc_res <- all_preds %>% select(lon, lat, datetime, resid_poc = residuals)

# plankton residuals from env
load("data/10.all_preds_plankton_best_from_env.Rdata")
plankton_res <- plankton_res %>% select(lon, lat, datetime, contains("resid"))

# store together
df <- left_join(poc_res, plankton_res, by = join_by(lon, lat, datetime))

output_filename <- "data/11.all_preds_poc_res_from_plankton_res.Rdata"
```

## Data inspection

### Response variable

Let’s have a look at our response variable: POC value. We do not need to inspect response variables as the model we are going to use are robust to non-normal distributions.

```{r poc_hist}
df %>% ggplot() + geom_histogram(aes(x = resid_poc), bins = 100)
```

Nice normal distribution.

### Explanatory variables

Select explanatory variables.

```{r sel_var}
# Explanatory variables
exp_vars <- df %>% select(resid_mo_ric, resid_ta_div, resid_ta_eve) %>% colnames()
# Metadata variables
meta_vars <- df %>% select(lon, lat, datetime) %>% colnames()
```

Let’s run a quick PCA. This allows us to have at correlations within our datasets as well as the main axes of variance.

```{r pca_exp}
# Need to remove lon and lat and to scale because units differ between variables
df_pca <- df %>% select(resid_poc, all_of(exp_vars))
pca_all <- FactoMineR::PCA(df_pca, quanti.sup = 1, scale.unit = TRUE, graph = FALSE)
```

Plot eigenvalues.

```{r pca_eig}
eig <- pca_all$eig %>%
  as.data.frame() %>%
  rownames_to_column(var = "comp") %>%
  as_tibble() %>%
  mutate(
    comp = str_remove(comp, "comp "),
    comp = as.numeric(comp),
    comp = as.factor(comp)
  ) %>%
  rename(var = `percentage of variance`, cum_var = `cumulative percentage of variance`)

eig %>%
  ggplot() +
  geom_col(aes(x = comp, y = var)) +
  theme_classic() +
  scale_y_continuous(expand = c(0, 0)) +
  labs(x = "PC", y = "Explained variance")
```

Plot first plane.

```{r pca_plot_var}
#| fig-column: body-outset
#| out-width: 100%
plot(pca_all, choix="var", axes = c(1, 2))
```

Extract coordinates in the PCA space.

```{r pca_ind}
## Get coordinates of individuals
inds <- pca_all$ind$coord %>% as_tibble()
# Set nice names for columns
colnames(inds) <- str_c("dim", paste(c(1:ncol(inds))))
# And join with initial dataframe of objects
inds <- df %>%
  bind_cols(inds)
```

Plot maps of profiles.

```{r pca_map_1}
#| fig-column: body-outset
#| out-width: 100%
ggmap(inds, "dim1", type = "point", palette = div_pal)
```

```{r pca_map_2}
#| fig-column: body-outset
#| out-width: 100%
ggmap(inds, "dim2", type = "point", palette = div_pal)
```

## Data split

Let’s take 80% of the data to fit the model, and keep 20% to test the model at the very end. We stratify our split by `poc_log`, the response value, so that both training and testing sets are representative of the whole range of values.

```{r data_split}
# Train VS test, stratified
df_split <- initial_split(df, prop = 0.8, strata = resid_poc)
df_train <- training(df_split)
df_test <- testing(df_split)
```

To evaluate model performance during training, we’ll use cross-validation on the training set, with 10 folds, again stratified by `poc_log`.

```{r data_cv}
# Cross-validation, 10 folds, stratified
df_folds <- vfold_cv(df_train, v = 10, strata = resid_poc)
```

## Model definition

Let’s define a XGBoost regression model, with tunable hyperparameters:

-   `trees`: number of trees

-   `tree_depth`: maximum depth (i.e. number of splits) in a tree

-   `min_n`: minimum number of objects in a node to split further

-   `learn_rate`

```{r def_mod}
# Define a xgboost model with hyperparameters to tune
xgb_spec <- boost_tree(
  trees = tune(),
  tree_depth = tune(),
  min_n = tune(),
  #loss_reduction = tune(),
  #sample_size = tune(),
  #mtry = tune(),
  learn_rate = tune()
) %>%
  set_mode("regression") %>%
  set_engine("xgboost", num.threads = n_cores)
extract_parameter_set_dials(xgb_spec)
```

The model will be run on `r n_cores` cores in parallel.

## Recipe

Let’s use a recipe to define the model formula and assign roles to variables. We want to predict `poc_log` from all other variables expect `lon`, `lat` and original `poc` values, yet it is convenient to keep these in the dataset.

```{r def_recipe}
# Generate formula from list of explanatory variables
# NB: we also add metadata variables and untransformed "poc" for technical reasons but these will not be used to fit the model
xgb_form <- as.formula(paste("resid_poc ~ ", paste(c(meta_vars, exp_vars), collapse = " + "), sep = ""))

xgb_rec <- recipe(xgb_form, data = df_train) %>%
  update_role(lon, lat, datetime, new_role = "coords") # These variables can be retained in the data but not included in the model
summary(xgb_rec)
```

## Workflow

Set-up the workflow with the recipe and the model.

```{r def_workflow}
xgb_wflow <- workflow() %>%
  add_recipe(xgb_rec) %>%
  add_model(xgb_spec)
```

## Gridsearch

It is now time to perform the gridsearch to find the best hyperparameters for our model. We first need to define a grid with the hyperparameters we want to try. We use `grid_latin_hypercube` to generate a space-filling grid of size `30`.

```{r def_grid}
xgb_grid <- grid_latin_hypercube(
  trees(),
  learn_rate(),
  tree_depth(),
  min_n(),
  #loss_reduction(),
  #sample_size = sample_prop(),
  #finalize(mtry(), df_train),
  size = 30
)
xgb_grid
```

Tune the grid to find the best hyperparameters, in parallel. This is done on our cross-validation folds: for each fold, a model is fitted on training data and evaluated on validation data.

```{r gridsearch}
doParallel::registerDoParallel()
xgb_res <- tune_grid(
  xgb_wflow,
  resamples = df_folds,
  grid = xgb_grid,
  control = control_grid(save_pred = TRUE)
)
autoplot(xgb_res)
```

Select the best set of hyperparameters based on `rmse` value.

```{r sel_best}
show_best(xgb_res, metric = "rmse")
best_xgb <- select_best(xgb_res, metric = "rmse")
```

## Finalize

Let’s now finalize our model with the selected set of hyperparameters.

```{r final_mod}
final_xgb <- finalize_workflow(
  xgb_wflow,
  best_xgb
)
```

Fit the model on training data.

```{r final_fit}
final_res <- fit(final_xgb, df_train)
```

Predict test data, and apply `exp` transformation as we predict the `log` of `poc` values. Still, we will evaluate model performances with predictions of `poc_log` as it is closer to a normal distribution.

```{r pred_test}
test_preds <- augment(final_res, new_data = df_test) %>%
  mutate(split = "test", .before = lon) %>% # tag split
  rename(pred_resid_poc = .pred)
```

## Model evaluation

### Prediction metrics

Let’s have a look at prediction metrics on the test set.

```{r eval_test}
rmse(test_preds, truth = resid_poc, estimate = pred_resid_poc)
rsq(test_preds, truth = resid_poc, estimate = pred_resid_poc)
```

### Prediction VS truth

Plot predictions VS truth, both in the log-transformed space and in the non log-transformed space.

```{r pred_vs_truth}
test_preds %>%
  ggplot() +
  geom_point(aes(x = resid_poc, y = pred_resid_poc)) +
  geom_abline(slope = 1, color = "red") +
  labs(title = "Pred VS truth")
```

## All predictions

```{r pred_all}
# Predict training set
train_preds <- augment(final_res, new_data = df_train) %>% 
  mutate(split = "train", .before = lon) %>% # tag split
  rename(pred_resid_poc = .pred)

# Join predictions for test and training sets
all_preds <- bind_rows(test_preds, train_preds)

# Save all predictions
save(all_preds, file = output_filename)
```

::: {.callout-note icon="false"}
## Conclusion

POC residuals cannot be predicted from plankton residuals.
:::
